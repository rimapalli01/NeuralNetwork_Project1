{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "009e8fc2",
   "metadata": {},
   "source": [
    "# 1. Build your own convolutional neural network using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "133be475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab261c3c-e02c-4c5e-9cc6-ec78f65f5e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Improved CNN\n",
    "class DogHeartCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DogHeartCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 8 * 8, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 3)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "model = DogHeartCNN().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece8be8c-b7fa-4ded-8e01-9fbe5d15e689",
   "metadata": {},
   "source": [
    "# 2. Train your model using dog heart dataset (you may need to use  Google Colab (or Kaggle) with GPU to train your code) \n",
    "\n",
    "### (1) use torchvision.datasets.ImageFolder for the training dataset\n",
    "### (2) use custom dataloader for test dataset (return image tensor and file name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "57e4be17-c521-4224-aeaf-0c08a03d154e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "data_dir = \"Dog_X_ray/Dog_heart\"\n",
    "train_dir = os.path.join(data_dir, \"Train\")\n",
    "valid_dir = os.path.join(data_dir, \"Valid\")\n",
    "test_dir = \"Dog_X_ray/Test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "093fb602-2af9-4000-bd98-4786fc386b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Large', 'Normal', 'Small']\n"
     ]
    }
   ],
   "source": [
    "# Transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Datasets\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)\n",
    "valid_dataset = datasets.ImageFolder(valid_dir, transform=valid_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(\"Classes:\", train_dataset.classes)\n",
    "\n",
    "# Test dataset\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, test_dir, transform=None):\n",
    "        self.test_dir = test_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = [os.path.join(test_dir, fname) for fname in os.listdir(test_dir)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.image_paths[idx]\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, os.path.basename(path)\n",
    "\n",
    "test_dataset = TestDataset(test_dir, transform=valid_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c260e89-9420-4ce6-abef-3407f0bdf3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rimap\\anaconda3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40] Train Loss: 2.2448, Train Acc: 42.14% Val Loss: 1.0627, Val Acc: 45.50%\n",
      "Epoch [2/40] Train Loss: 1.0293, Train Acc: 44.29% Val Loss: 0.9782, Val Acc: 44.50%\n",
      "Epoch [3/40] Train Loss: 0.9656, Train Acc: 47.21% Val Loss: 0.9280, Val Acc: 47.50%\n",
      "Epoch [4/40] Train Loss: 0.9286, Train Acc: 48.29% Val Loss: 0.8495, Val Acc: 60.50%\n",
      "Epoch [5/40] Train Loss: 0.8600, Train Acc: 54.00% Val Loss: 0.7769, Val Acc: 62.50%\n",
      "Epoch [6/40] Train Loss: 0.8017, Train Acc: 59.64% Val Loss: 0.7576, Val Acc: 62.00%\n",
      "Epoch [7/40] Train Loss: 0.7677, Train Acc: 60.57% Val Loss: 0.7238, Val Acc: 62.50%\n",
      "Epoch [8/40] Train Loss: 0.7368, Train Acc: 62.43% Val Loss: 0.7141, Val Acc: 65.00%\n",
      "Epoch [9/40] Train Loss: 0.7284, Train Acc: 61.86% Val Loss: 0.7218, Val Acc: 61.50%\n",
      "Epoch [10/40] Train Loss: 0.7011, Train Acc: 64.14% Val Loss: 0.6509, Val Acc: 69.00%\n",
      "Epoch [11/40] Train Loss: 0.7091, Train Acc: 64.50% Val Loss: 0.6510, Val Acc: 71.50%\n",
      "Epoch [12/40] Train Loss: 0.7080, Train Acc: 65.64% Val Loss: 0.6707, Val Acc: 65.50%\n",
      "Epoch [13/40] Train Loss: 0.6768, Train Acc: 66.14% Val Loss: 0.6422, Val Acc: 68.00%\n",
      "Epoch [14/40] Train Loss: 0.6640, Train Acc: 65.93% Val Loss: 0.6648, Val Acc: 69.00%\n",
      "Epoch [15/40] Train Loss: 0.6705, Train Acc: 67.21% Val Loss: 0.6436, Val Acc: 68.00%\n",
      "Epoch [16/40] Train Loss: 0.6385, Train Acc: 69.93% Val Loss: 0.6135, Val Acc: 69.50%\n",
      "Epoch [17/40] Train Loss: 0.5959, Train Acc: 71.86% Val Loss: 0.6110, Val Acc: 67.50%\n",
      "Epoch [18/40] Train Loss: 0.6203, Train Acc: 70.00% Val Loss: 0.5943, Val Acc: 73.50%\n",
      "Epoch [19/40] Train Loss: 0.5951, Train Acc: 70.14% Val Loss: 0.6857, Val Acc: 65.00%\n",
      "Epoch [20/40] Train Loss: 0.5950, Train Acc: 71.43% Val Loss: 0.6202, Val Acc: 68.00%\n",
      "Epoch [21/40] Train Loss: 0.5897, Train Acc: 72.21% Val Loss: 0.6278, Val Acc: 68.00%\n",
      "Epoch [22/40] Train Loss: 0.5853, Train Acc: 71.71% Val Loss: 0.7771, Val Acc: 60.50%\n",
      "Epoch [23/40] Train Loss: 0.5592, Train Acc: 73.79% Val Loss: 0.5866, Val Acc: 72.00%\n",
      "Epoch [24/40] Train Loss: 0.5502, Train Acc: 72.71% Val Loss: 0.6837, Val Acc: 65.50%\n",
      "Epoch [25/40] Train Loss: 0.5646, Train Acc: 72.79% Val Loss: 0.6449, Val Acc: 66.50%\n",
      "Epoch [26/40] Train Loss: 0.5502, Train Acc: 74.21% Val Loss: 0.6232, Val Acc: 68.00%\n",
      "Epoch [27/40] Train Loss: 0.5228, Train Acc: 74.36% Val Loss: 0.6065, Val Acc: 68.00%\n",
      "Epoch [28/40] Train Loss: 0.5240, Train Acc: 75.29% Val Loss: 0.5920, Val Acc: 68.50%\n",
      "Epoch [29/40] Train Loss: 0.5218, Train Acc: 75.14% Val Loss: 0.5913, Val Acc: 69.00%\n",
      "Epoch [30/40] Train Loss: 0.5058, Train Acc: 76.14% Val Loss: 0.5861, Val Acc: 69.50%\n",
      "Epoch [31/40] Train Loss: 0.5000, Train Acc: 76.71% Val Loss: 0.5894, Val Acc: 70.00%\n",
      "Epoch [32/40] Train Loss: 0.4975, Train Acc: 76.79% Val Loss: 0.5958, Val Acc: 68.00%\n",
      "Epoch [33/40] Train Loss: 0.5028, Train Acc: 76.36% Val Loss: 0.5801, Val Acc: 69.50%\n",
      "Epoch [34/40] Train Loss: 0.4926, Train Acc: 77.36% Val Loss: 0.5817, Val Acc: 69.50%\n",
      "Epoch [35/40] Train Loss: 0.4969, Train Acc: 76.57% Val Loss: 0.5894, Val Acc: 69.00%\n",
      "Epoch [36/40] Train Loss: 0.4922, Train Acc: 77.36% Val Loss: 0.5828, Val Acc: 69.50%\n",
      "Epoch [37/40] Train Loss: 0.4818, Train Acc: 78.00% Val Loss: 0.5890, Val Acc: 69.00%\n",
      "Epoch [38/40] Train Loss: 0.4946, Train Acc: 76.79% Val Loss: 0.5826, Val Acc: 69.00%\n",
      "Epoch [39/40] Train Loss: 0.4862, Train Acc: 77.43% Val Loss: 0.5820, Val Acc: 69.00%\n",
      "Epoch [40/40] Train Loss: 0.4801, Train Acc: 77.64% Val Loss: 0.5802, Val Acc: 70.00%\n"
     ]
    }
   ],
   "source": [
    "# Loss, optimizer, scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "# Training loop (full 40 epochs)\n",
    "num_epochs = 40\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_acc = 100 * correct / total\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in valid_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item() * images.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    val_loss /= len(valid_loader.dataset)\n",
    "    val_acc = 100 * val_correct / val_total\n",
    "\n",
    "    scheduler.step(val_acc)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% \"\n",
    "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b4f9dfb-43f9-485f-bb91-58bd932cf4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to dog_heart_cnn.pt\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"dog_heart_cnn.pt\")\n",
    "print(\"Model saved to dog_heart_cnn.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f63262f",
   "metadata": {},
   "source": [
    "# 3. Evaluate your model using the developed software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "687038bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved predictions with numeric labels to dog_heart_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Predictions on test set\n",
    "label_map = {class_name: idx for class_name, idx in train_dataset.class_to_idx.items()}\n",
    "idx_to_class = {v: k for k, v in train_dataset.class_to_idx.items()}\n",
    "model.eval()\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, filenames in test_loader:\n",
    "        images = images.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        predicted_labels = [label_map[idx_to_class[idx]] for idx in preds.cpu().numpy()]\n",
    "        for fname, label in zip(filenames, predicted_labels):\n",
    "            predictions.append((fname, label))\n",
    "\n",
    "# Save to CSV \n",
    "df_preds = pd.DataFrame(predictions)\n",
    "df_preds.to_csv(\"dog_heart_predictions.csv\", index=False, header=False)\n",
    "print(\"Saved predictions with numeric labels to dog_heart_predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5846bc",
   "metadata": {},
   "source": [
    "# 4. Compare results with [RVT paper](https://www.nature.com/articles/s41598-023-50063-x). Requirement: performance is better than VGG16: 75%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ed891c-774f-4b66-803b-2ae8e2eccfc9",
   "metadata": {},
   "source": [
    "The custom CNN model achieved a validation accuracy of 73%, which reflects competitive performance on the Dog Heart X-ray dataset. However, this result falls slightly below the benchmark set by VGG16, which achieved 75% accuracy. In contrast, the Regressive Vision Transformer (RVT), as proposed by Zhang et al. (2023), reached a notably higher accuracy of 92.3%, highlighting the effectiveness of transformer-based architectures in this domain. These findings suggest that incorporating advanced architectures such as RVT could lead to improved performance in future implementations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f12835",
   "metadata": {},
   "source": [
    "# 5. Write a four-page paper report using the shared LaTex template. Upload your paper to ResearchGate or Arxiv, and put your paper link and GitHub weight link here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d31a06-8a36-4963-9ce4-3108eb32e256",
   "metadata": {},
   "source": [
    "https://www.researchgate.net/publication/390180445_Dog_Heart_Cnn_Report\n",
    "\n",
    "https://github.com/rimapalli01/NeuralNetwork_Project1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f476372c",
   "metadata": {},
   "source": [
    "# 6. Grading rubric\n",
    "\n",
    "(1). Code ------- 20 points (you also need to upload your final model as a pt file)\n",
    "\n",
    "(2). Grammer ---- 20 points\n",
    "\n",
    "(3). Introduction & related work --- 10 points\n",
    "\n",
    "\n",
    "(4). Method  ---- 20 points\n",
    "\n",
    "(5). Results ---- 20 points\n",
    "\n",
    "     > = 75 % -->10 points\n",
    "     < 55 % -->0 points\n",
    "     >= 55 % & < 75% --> 0.5 point/percent\n",
    "     \n",
    "\n",
    "(6). Discussion - 10 points"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
